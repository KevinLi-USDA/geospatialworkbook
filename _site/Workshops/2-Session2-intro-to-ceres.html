<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.11.2 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Introduction to Ceres HPC - Geospatial Workbook</title>
<meta name="description" content="Tutorial on Informatics for Geospatial Information">


  <meta name="author" content="Kerrie Geil">


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Geospatial Workbook">
<meta property="og:title" content="Introduction to Ceres HPC">
<meta property="og:url" content="http://localhost:4000/Workshops/2-Session2-intro-to-ceres.html">




  <meta property="og:image" content="http://localhost:4000/assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg">



  <meta name="twitter:site" content="@isugif">
  <meta name="twitter:title" content="Introduction to Ceres HPC">
  <meta name="twitter:description" content="Tutorial on Informatics for Geospatial Information">
  <meta name="twitter:url" content="http://localhost:4000/Workshops/2-Session2-intro-to-ceres.html">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://localhost:4000/assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg">
  

  
    <meta name="twitter:creator" content="@someone">
  







  

  


<link rel="canonical" href="http://localhost:4000/Workshops/2-Session2-intro-to-ceres.html">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Geospatial Workbook Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E6BZVYF8ZY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E6BZVYF8ZY');
</script>

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Geospatial Workbook</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/about.html" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/list.html" >Index</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/glossary.html" >Glossary</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/people.html" >People</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/contributing.html" >Contribute</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  











<div class="page__hero--overlay"
  style="background-color: 444444; background-image: url('/assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Introduction to Ceres HPC

        
      </h1>
      
      
      
    </div>
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/people/KerrieGeil.png" alt="Kerrie Geil" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Kerrie Geil</h3>
    
    
      <p class="author__bio" itemprop="description">
        Kerrie is an ARS SCINet postdoc in the research group of Dr. Deb Peters in Las Cruces, NM. Her M.S. and Ph.D. degrees are in Atmospheric Sciences and her research background is in climate modeling.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      
        <li>
          <a href="mailto:mailto:someone@iastate.edu">
            <meta itemprop="email" content="mailto:someone@iastate.edu" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      
        <li>
          <a href="https://twitter.com/someone" itemprop="sameAs">
            <i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

<!-- Create a 2nd author for tutorials -->



<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/people/RowanGaffney.jpg" alt="Rowan Gaffney" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Rowan Gaffney</h3>
    
    
      <p class="author__bio" itemprop="description">
        Rowan is a physical scientist in the Rangeland Resource & Systems Research Unit in Fort Collins, CO. He specializes in analyzing large, multidimensional geospatial data using a variety of approaches from machine learning to numerical analysis.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      
        <li>
          <a href="mailto:mailto:rowan.gaffney@usda.gov">
            <meta itemprop="email" content="mailto:rowan.gaffney@usda.gov" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      
        <li>
          <a href="https://twitter.com/someone" itemprop="sameAs">
            <i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      

      

      

      

      

      

      
        <li>
          <a href="https://github.com/https://github.com/rmg55" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Introduction to Ceres HPC">
    
    
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
        <!--# Session 2 Tutorial:-->

<h1 id="introduction-to-the-ceres-high-performance-computing-system-environment">Introduction to the Ceres High-Performance Computing System Environment</h1>

<p>This page was modified from Session 2 of the SCINet Geospatial Workshop 2020. Ceres HPC Resources have been used to publish scientific papers:</p>

<ul>
  <li><a href="https://www.ncbi.nlm.nih.gov/pmc/?term=ceres+scinet">List of articles on PMC which acknowledged Ceres HPC</a></li>
</ul>

<p><strong>Learning Goals:</strong></p>

<ul>
  <li>understand what an HPC system is and when to use one</li>
  <li>introduction to the USDA-ARS Ceres HPC system on SCINet</li>
  <li>access the Ceres with Secure Shell at the command line and with the JupyterHub web interface</li>
  <li>create a simple Jupyter notebook</li>
  <li>execute basic linux commands</li>
  <li>run an interactive computing session on Ceres</li>
  <li>write and run a SLURM batch script on Ceres
<br /><br /></li>
</ul>

<hr />

<h2 id="contents">Contents</h2>

<p><a href="#session-rules">Session Rules</a></p>

<p><a href="#the-scinet-website">The SCINet Website</a></p>

<p><a href="#high-performance-computing-hpc-system-basics">High-Performance Computing (HPC) System Basics</a></p>
<ul>
  <li><a href="#what-is-an-hpc-system">What is an HPC system?</a></li>
  <li><a href="#why-use-an-hpc-system">Why use an HPC system?</a></li>
  <li><a href="#hpc-terminology">HPC terminology</a></li>
</ul>

<p><a href="#usda-ars-hpc-system-details">USDA-ARS HPC System Details</a></p>
<ul>
  <li><a href="#the-ceres-hpc-system">The Ceres HPC System</a></li>
  <li><a href="#other-scinet-hpc-systems">Other SCINet HPC Systems</a></li>
</ul>

<p><a href="#ceres-hpc-login-with-secure-shell-ssh">Ceres HPC Login with Secure Shell (SSH)</a></p>

<!--
[Ceres HPC Login with JupyterHub](#ceres-hpc-login-with-jupyterhub)
  - [Tour of JupyterLab](#tour-of-jupyterlab)
  - [Jupyter Notebook Basics](#jupyter-notebook-basics)

[Basic Linux Commands](#basic-linux-commands)
-->

<p><a href="#interactive-computing-vs-batch-computing">Interactive Computing vs Batch Computing</a></p>
<ul>
  <li><a href="#interactive-computing-on-ceres">Interactive Computing on Ceres</a></li>
  <li><a href="#batch-computing-on-ceres">Batch Computing on Ceres</a></li>
</ul>

<p><a href="#submitting-a-compute-job-with-a-slurm-batch-script">Submitting a Compute Job with a SLURM Batch Script</a></p>

<hr />

<!--
## Session Rules

**GREEN LIGHT, RED LIGHT** - Use the Zoom participant feedback indicators to show us if you are following along successfully as well as when you need help. To access participant feed back, click on the "Participants" icon to open the participants pane/window. Click the green "yes" to indicate that you are following along successfully, click the red "no" to indicate when you need help. Ideally, you will have either the red or green indicator displayed for yourself throughout the entire tutorial. We will pause every so often to work through solutions for participants displaying a red light.

**CHAT QUESTIONS/COMMENTS TAKE FIRST PRIORITY** - Chat your questions/comments either to everyone (preferred) or to the chat moderator (Rowan or Kerrie - whoever is not actively presenting) privately to have your question/comment read out loud anonamously. We will address chat questions/comments first and call on people who have written in the chat before we take questions from raised hands.

**SHARE YOUR VIDEO WHEN SPEAKING** - If your internet plan/connectivity allows, please share your video when speaking.

**KEEP YOURSELF ON MUTE** - Please mute yourself unless you are called on.
<br><br>

---
-->

<h2 id="the-scinet-website">The SCINet Website</h2>

<p>The <a href="https://scinet.usda.gov/">SCINet Website</a> is the source of much of the material presented in this tutorial. Use the SCINet website to request SCINet accounts, access SCINet/HPC user guides, get computing help or other support, and find out about upcoming and previous computational training events.</p>

<p><br /></p>

<p><a href="#session-2-tutorial">return to top of page</a>
<br /><br /></p>

<hr />

<h2 id="high-performance-computing-hpc-system-basics">High-Performance Computing (HPC) System Basics</h2>

<h3 id="what-is-an-hpc-system">What is an HPC System?</h3>

<p>A High Performance Computing (HPC) system provides a computational environment that can processes data and perform complex computations at high speeds. Generally HPC systems consists of 3 components:</p>

<ol>
  <li>
    <p><em>Compute nodes</em> (servers) that can provide a consistent environment across the system (similiar OS, software, etc…).</p>
  </li>
  <li>
    <p><em>Data storage</em>, generally a parallel file system, that supports high I/O throughput.</p>
  </li>
  <li>
    <p><em>Highspeed network</em> to allow for effecient communication and data transfer across compute nodes.</p>
  </li>
</ol>

<p>For specific details about the Ceres system see: <a href="https://scinet.usda.gov/guide/ceres/#technical-overview">https://scinet.usda.gov/guide/ceres/#technical-overview</a></p>

<h3 id="why-use-an-hpc-system">Why use an HPC System?</h3>

<p>HPC systems can provide the compute infrastructure that allow researchers to improve or make possible computationally intensive analyses/workflows. However, developing analyses to run on HPC systems involve a non-trivial amount of overhead. Therefore, you should first evaluate if SCINet is an appropriate avenue for your research. Typically, analyses that are well-suited for SCINet are:</p>

<ul>
  <li>CPU intensive workloads</li>
  <li>high memory workloads</li>
</ul>

<p>Additional considerations are:</p>

<ul>
  <li>Are my analyses already optimized?</li>
  <li>Will I need to parallelize my analyses (typical for CPU intensive workloads)?</li>
  <li>Will I require more than a single node of compute power (ie. distributed computing)?</li>
</ul>

<h3 id="hpc-terminology">HPC Terminology</h3>

<p><strong>SSH</strong> - Secure Shell is a network protocol that allows remote access over un-secure networks. We will use SSH to access the Ceres login node.</p>

<p><strong>shell</strong> - a shell is what provides you an interface to the unix operating system. It’s where we type commands and run programs. The default shell on Ceres is called bash.</p>

<p><strong>login/compute node</strong> - Nodes refer to the individual servers that compose an HPC system. The login node is the node/server that users are sent to when they SSH to the system. The compute nodes (typically the bulk of the HPC nodes) are designed for running the computationally intensive workloads. There can be many different types of compute nodes within a HPC system (i.e. standard, high memory, gpu, etc…).</p>

<p><strong>core/logical core</strong> - Cores (or CPU) are the computational processing component within a computer. Most modern cores have hyperthreading, which allow a single core to process two tasks simultaneously. Therefore, logical cores refer to the number apparent (not physical) cores in a system. For most modern systems, a single core will equate to two logical cores.</p>

<p><strong>batch/interactive computing</strong> - Batch computing referes to workflows that require no user interaction once they are underway/submitted. Interactive computing typically involves processing commands/transactions one at a time and requires input from the user.</p>

<p><strong>SLURM/job scheduler</strong> - HPC systems generally have software to allocate resources (nodes, cores, memory, etc…) in a fair and consistent manner to the users. These systems are generally refered to as <em>job schedulers</em>. A common job scheduling software used in HPC systems is <a href="https://slurm.schedmd.com/documentation.html">SLURM</a>.</p>

<p><br /></p>

<p><a href="#session-2-tutorial">return to top of page</a>
<br /><br /></p>

<hr />

<h2 id="usda-ars-hpc-system-details">USDA-ARS HPC System Details</h2>

<p>The <a href="https://scinet.usda.gov/about/compute">Computer Systems</a> page of the SCINet website gives a brief summary of the USDA-ARS HPC systems.
<br /><br /></p>

<h3 id="the-ceres-hpc-system">The Ceres HPC System</h3>

<p>The <a href="https://scinet.usda.gov/guide/ceres/">Ceres User Manual</a> and <a href="https://scinet.usda.gov/guide/quickstart">Ceres Quick Start Guide</a> contain most of the information you could want to know about the Ceres HPC.
<br /><br /></p>

<p><strong>System Configuration</strong></p>

<p>The operating system running on Ceres is CentOS and the job scheduler is SLURM. See <a href="https://scinet.usda.gov/guide/ceres/#system-configuration">System Configuration in the Ceres User Manual</a> for more.
<br /><br /></p>

<p><strong>Nodes</strong></p>

<p>When you SSH into Ceres you are connecting to the login node. The login node should be used for navigating your directories, organizing your files, and running very minor scripts. All computing on Ceres should be done on compute nodes. <strong>DON’T RUN YOUR COMPUTE SCRIPTS OR INSTALL SOFTWARE ON THE LOGIN NODE AS IT SLOWS THE NODE DOWN FOR EVERYONE.</strong></p>

<p>When you use JupyterHub to login to Ceres you are placed on a compute node, not a login node.</p>

<p>There are 5000+ compute cores (10000 logical cores), 65 TB of total RAM, 250TB of total local storage, and 4.7 PB of shared storage available on the Ceres HPC. See the <a href="https://scinet.usda.gov/guide/ceres/#technical-overview">Technical Overview in the Ceres User Manual</a> for more.
<br /><br /></p>

<p><strong>Partitions/Queues</strong></p>

<p>All Ceres users have access to the “community partitions”- short, medium, long, long60, mem, longmem, mem768, debug. Each different partition has different capabilities (e.g. regular memory versus high memory nodes) and resource restrictions (e.g. time limits on jobs). The short partition is the default. See <a href="https://scinet.usda.gov/guide/ceres/#partitions-or-queues">Partitions or Queues in the Ceres User Manual</a> for more.</p>

<p>Some research groups have purchased their own nodes which are placed on “priority partitions” that the research group has first priority to use. Other Ceres users have lower priority access to many of these partitions (the “-low” and “scavenger” partitions). However, the “-low” partitions have a compute time limit of 2 hours and while you can run jobs for much longer on the “scavenger” partitions, you run the risk of having your job killed at any moment if a higher priority user needs access to the nodes.
<br /><br /></p>

<p><strong>Directory structure and data storage</strong></p>

<p>There are 3 places to store your codes and data persistently on Ceres:</p>

<ul>
  <li>home directory, /home/firstname.lastname/ - Everyone has a small private home directory. You are automatically put in this directory when you log in. Home directories are backed up. You shouldn’t run computations from your home directory.</li>
  <li>project directory, /lustre/project/your_project_name/ - You must <a href="https://scinet.usda.gov/support/request-storage">apply for each project directory</a>. This is where you run scripts from and where your datasets, codes, and results should live. Project directories are not backed up.</li>
  <li>keep directory, /KEEP/your_project_name/ - Each of your projects will also have folder in the /KEEP directory. This folder is backed up nightly. Copy your most important project files to your /KEEP folder to ensure they are backed up. Compute jobs shouldn’t be run from /KEEP directories.</li>
</ul>

<p>Temporarily share files locally with other users:</p>

<ul>
  <li>project shared files directory, /lustre/project/shared_files/ - Everyone has access to the shared files folder in the project directory. You can share files with other users by creating a folder inside this directory a copying your files there, although there is a 5GB limit. This is not a permanent storage location for your files.</li>
</ul>

<p>See more about directories and data storage in <a href="https://scinet.usda.gov/guide/quickstart#data-storage">Data Storage in the Quick Start Guide</a> and in <a href="https://scinet.usda.gov/guide/ceres/#quotas-on-home-and-project-directories">Quotas in Home and Project Directories in the Ceres User Manual</a>.
<br /><br /></p>

<p><strong>User Compute Limitations</strong></p>

<p>There is currently no quota system for keeping track of how many jobs you run or how much compute time you’ve used on Ceres like there is on University and National Lab HPC systems. However, there is a fair share policy which means super users who run many jobs and take up a lot of compute time are downgraded in priority compared to other users.</p>

<p>The individual user compute limitations are:</p>
<ul>
  <li>400 cores per user (across all your running jobs)</li>
  <li>1512 GB memory per user (across all your running jobs)</li>
  <li>100 jobs per user running simultaneously
<br /><br /></li>
</ul>

<p><strong>Software on Ceres</strong></p>

<p>There is plenty of software on Ceres that you can access through the module system. See the <a href="https://scinet.usda.gov/guide/software">Software Overview</a> for more.</p>

<p>Users can also install their own software using the Anaconda package and environment management software module. See the <a href="https://scinet.usda.gov/guide/conda/">Conda Guide</a> from more.</p>

<p>If you don’t want to use Conda, Ceres is also set up for R and Perl users to download packages. See the <a href="https://scinet.usda.gov/guide/packageinstall/">Guide to Installing R, Perl, and Python Packages</a> for more (although we recommend that Python coders use Conda).</p>

<p>Lastly, if none of the above methods of accessing software work for your particular software needs, you can request that the SCINet Virtual Research Support Core install software on the system for you. This is the method of last resort though because it takes a few weeks and requires an agency-level security review. See the <a href="https://scinet.usda.gov/support/request-software">Request Software page</a> for more.
<br /><br /></p>

<p><strong>Getting data on/off</strong></p>

<p>There are multiple ways of getting data on and off of the Ceres HPC system. See the <a href="https://scinet.usda.gov/guide/file-transfer/">SCINet File Transfer Guide</a> for more.</p>

<ul>
  <li>for data on the web, download directly to Ceres using tools like wget</li>
  <li>for data on your local machine the recommended method is Globus</li>
  <li>for large local data, you can:
    <ol>
      <li>utilize the I2 connection at your ARS location or</li>
      <li>ship to VRSC and they will upload for you
<br /><br /></li>
    </ol>
  </li>
</ul>

<p><strong>User support from the Virtual Research Support Core (VRSC)</strong></p>

<p>The VRSC is comprised of Iowa State University and ARS staff who manage the Ceres HPC system and provide support to users. See more on the <a href="https://scinet.usda.gov/support/vrsc/">SCINet VRSC Support page</a>.
<br /><br /></p>

<p><strong>Ceres HPC Best Practices</strong></p>
<ul>
  <li>nothing serious should be run on the login node</li>
  <li>run compute jobs from project directories</li>
  <li>install software on a compute node (salloc)</li>
  <li>install software (including Conda environments) in your home or /KEEP directories</li>
  <li>use <a href="https://scinet.usda.gov/guide/ceres/#local-scratch-space-on-large-memory-nodes">node local scratch space</a> for faster i/o with large datasets (stage your data to /local/scratch, $TMPDIR)</li>
  <li>use your /KEEP directory to ensure important data/codes are backed up</li>
  <li>for short heavy compute jobs (less than 2hrs) go for the brief-low partition (more cores per node and more memory)</li>
  <li><a href="https://scinet.usda.gov/guide/quickstart#scinet-citationacknowledgment-in-publications">acknowledge SCINet in your pubs</a>!
<br /><br /></li>
</ul>

<h3 id="other-scinet-hpc-systems">Other SCINet HPC Systems</h3>
<p>There are two other HPC Systems coming to SCINet soon. Summaries of the systems will be posted to the SCINet website <a href="https://scinet.usda.gov/about/compute">computing systems page</a>.</p>

<p><br /></p>

<p><a href="#session-2-tutorial">return to top of page</a>
<br /><br /></p>

<hr />

<h2 id="ceres-hpc-login-with-secure-shell-ssh">Ceres HPC Login with Secure Shell (SSH)</h2>

<p>First, let’s login to our SCINet (Ceres) accounts with SSH. You should have already successfully logged in this way at least once before today by following the instructions sent to you when your SCINet account was approved. The <a href="https://scinet.usda.gov/guide/quickstart#accessing-scinet">Quick Start Guide</a> has instructions for SSH’ing to Ceres from Windows, Mac, and Linux Machines.</p>

<p>If you haven’t yet set up a config file for SSH’ing to Ceres (we won’t cover it but instructions are at the Quick Start Guide link above) then:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-o</span> <span class="nv">TCPKeepAlive</span><span class="o">=</span><span class="nb">yes</span> <span class="nt">-o</span> <span class="nv">ServerAliveInterval</span><span class="o">=</span>20 <span class="nt">-o</span> <span class="nv">ServerAliveCountMax</span><span class="o">=</span>30 firstname.lastname@ceres.scinet.usda.gov
</code></pre></div></div>

<p>The keep alives are especially important for rural/satellite internet connections so that instantaneous breaks in service won’t terminate your connection to the HPC.</p>

<p>If you’ve set up your config file you can simply:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh whatever-you-named-the-host-in-your-config
</code></pre></div></div>

<p>If you are not on an ARS VPN, you will be asked for a 6-digit Google Authenticator code. See the <a href="https://scinet.usda.gov/guide/multifactor/">multi-factor authentication guide</a> for help. After entering the Google code, you will be asked to enter your password.</p>

<p>If you are on an ARS VPN, you will skip the Google authentication and be asked only for your password.</p>

<p>After a successful login you will see a list of all your quotas and used space.</p>

<p>If you can’t successfully login to your account, contact scinet_vrsc@usda.gov for assistance.</p>

<p>To sign out of Ceres just close your terminal or type <code class="language-plaintext highlighter-rouge">exit</code>.
<br /></p>

<p><a href="#session-2-tutorial">return to top of page</a>
<br /><br /></p>

<hr />

<!--
## Ceres HPC Login with JupyterHub

In summer 2020, a new way of accessing Ceres was set up with JupyterHub. JupyterHub allows access to Ceres through a web browser and spawns an instance of JupyterLab on a compute node.

Let's walk through how to login to Ceres using JupyterHub with the [Jupyter Guide](https://scinet.usda.gov/guide/jupyter).

### Tour of JupyterLab

The Launcher:
Click the plus sign on the left or go to File > New Launcher to see the launcher screen. From here you can open a Jupyter Notebook, a terminal, a textfile, a markdown file, and more.

Accessing your files:
Clicking on the folder icon on the far left will show you the files and folders in your home directory or the directory you listed in the JupyterLab spawner during the login process.

Access to software modules:
Clicking on the hexagon icon on the far left will show you all the software modules available on the Ceres HPC. Clicking load on any of the modules is equivalent to typing ```module load name-of-software``` (as described in the [Quick Start Guide](https://scinet.usda.gov/guide/quickstart#using-the-software-applications-on-ceres)). Note: there aren't a ton of software modules on Ceres that are relevant to the geospatial research community, so you likely won't need to use this feature often.


### Jupyter Notebook Basics

**Step 1: Open a Jupyter Notebook**

Click the launcher and launch a Python 3 notebook- notice the .ipynb file extension.

Notice how it says "Python 3" at the top right of the notebook. You are working in a Python 3 environment or "kernel". You could change this by clicking on "Python 3" and selecting a different kernel from the dropdown list in the pop-up box. Don't choose a different kernel for now, but note that this is where you could select a Conda environment that you have created. We will cover this in the Session 4 Tutorial on computational reproducibility.

**Step 2: Add a Raw Text Cell**

At the top of the notebook click "Code" and change it to "Raw". Click on the cell and type the following:
```bash
author: your name
date: today's date
description: my first jupyter notebook hello world
```

To execute the cell type Shift+Enter or click the run button at the top of the notebook (looks like "play").

**Step 3: Add a Markdown Cell**

Click inside the new cell that has appeared in your notebook, then at the top of the notebook click again on "Code" and change it to "Markdown". Click inside the Markdown cell and type:
```bash
# Make Notes in Your Codes Prettier Using Markdown
## add a subtitle

Write some pretty text.
```

Execute the Markdown cell.

Learn more about JupyterHub markdown syntax [here](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) or [here](https://www.ibm.com/support/knowledgecenter/en/SSGNPV_2.0.0/dsx/markd-jupyter.html). A quick Google search on "JupyterHub markdown cheat sheet" will reveal tons of helpful sites.

**Step 4: Add a Code Cell**

If you executed the previous cell, a new code cell should have automagically appeared. Type:
```bash
print('Hello, Jupyter world!')
```

and execute the cell.

Notice how the output appears right in the Jupyter notebook. You can also print tables and plot figures right in the Jupyter notebook... all your code, comments, and outputs will be in one place- inside the .ipynb file!


**Step 5: Delete a Cell**

Right click on the cell and choose "Delete Cells" or place your cursor in the cell and then click on the scissors icon at the top of the notebook.

**Step 6: Insert a Cell**

Insert a cell under the markdown cell by clicking on the markdown cell and then clicking the plus button at the top of the notebook (next to the scissors). Type:
```bash
print('How awesome is this?!')
```

and execute the cell.

**Step 7: Clear all Outputs**

At the top of JupyterLab click Kernel > Restart Kernel and Clear All Outputs, the click Restart in the pop-up window. All you outputs are now cleared.

**Step 8: Re-run all Cells**

At the top of JupyterLab click Run > Run All Cells

**Step 9: Save Your Work**

Actually, JupyterLab is autosaving your notebook as you work, but you'll want to name your file.

Right click on "Untitled.ipynb" either on the notebook tab or in the file browser on the left, then choose "Rename". In the pop-up window, name your file and click "Rename".

<br>

[return to top of page](#session-2-tutorial)
<br><br>

---
-->

<!-- NOTE: should be in intro unix

## Basic Linux Commands

Now we'll work through some basic linux commands. For more commands than we cover here see the [Unix Basics Tutorial from the bioinformatics workbook](https://bioinformaticsworkbook.org/Appendix/Unix/unix-basics-1.html#gsc.tab=0) created by Andrew Severin of Iowa State University/SCINet VRSC.

First an illustration of the following terms:
- **prompt**,
- **command**,
- **argument** (a.k.a options, flags),
- **standard out** (stdout),
- **standard error** (stderr)

![picture of terminal from bioinformaticsworkbook.org](/SCINET-GEOSPATIAL-RESEARCH-WG/images/andrews-graphic.png)
[image source](https://bioinformaticsworkbook.org/Appendix/Unix/unix-basics-1.html#gsc.tab=0)

<br><br>

**What directory am I in?**

**pwd** - print the path of the working directory (the directory that you are current in) to the screen (standard out)
```bash
pwd
```
stdout should show ```/home/firstname.lastname```. This directory can be accessed with ~, $HOME, or by typing only ```cd``` (all of which we will cover later).

<br><br>
**How to make a new directory (folder)**

**mkdir** - make a new directory
```bash
mkdir mynewdirectory
```

You have now created a new empty directory inside of your home directory. To see it we will use ```ls```


<br><br>

**How to list (see) files and directories**

**ls** - list directory contents
```bash
ls
```
You should see your new directory along with other files and directories that you have in your home directory.

To view all files and directories (including hidden ones that start with a ".") add an option/flag to the ```ls``` command:
```bash
ls -a
```
you will get the same result with:
```bash
ls --all
```
Note that many options are accessible using the long version which always starts with two dashes (--all) or using the abbreviated version which always starts with one dash (-a).

Also, you can add multiple options to commands:
```bash
ls -alh
```
will show you all files in your working directory in the long listing format (-l) and with file sizes listed in human-readable format (-h). The -l option shows you permissions, ownership, size, and last-modified date.

Here's a key to the long format ls -l:

![picture of long format file info](/SCINET-GEOSPATIAL-RESEARCH-WG/images/file-info.png)
[image source](https://linuxize.com/post/chmod-command-in-linux/)

We'll come back to how to change file or directory permissions.

<br><br>

**What options are available for each command?**

**man** - to view the reference manual for a command which will show the command format and all the available options.
```bash
man ls
```
So many options! Don't worry, you don't have to know all the options for every command. To view the entire manual page, use your up/down arrows to scroll or your pg up/pg dn buttons.

NOTE: If you haven't discovered it already, **YOUR MOUSE DOESN'T WORK AT THE COMMAND LINE!** Notice the -a, -l, and -h on the man page for the ls command.

To exit/quit the man page:
```bash
q
```

<br><br>

**Change Directory**

**cd** - change directory

Move from your home directory into your new directory with:
```bash
cd mynewdirectory
```

Look at what directory you are in now:
```bash
pwd
```

You should be at /home/firstname.lastname/mynewdirectory

To go up/back directory levels use "..":

```bash
cd ../..
```

takes you back two levels to the /home directory, where if you `ls` you will see the home directories all of users.

Go back to mynewdirectory with:

```bash
cd ~/mynewdirectory
```
Note how the "~" is a shortcut for /home/firstname.lastname. Another shortcut for your home directory is $HOME. Replacing ~ with $HOME in the above command would yield the exact same result.

To get back to your home directory from anywhere, just type cd with no arguments:

```bash
cd
```

<br><br>

**Creating a text file with nano**

**nano** - a terminal-based text editor. This means when you use nano, your prompt will dissapear and you will instead be editing a text file on your terminal screen. There are plenty of other terminal-based text editors but we won't cover them here.

First, change directory into mynewdirectory.

Then, open a new empty text file:

```bash
nano
```

You can recognize that you are now in the nano text editor due to the banners on the top and bottom of your terminal. The top banner will say the nano version, the bottom banner shows shortcut keys to help you use the editor.

Let's write in our new text file:

```text

1
dog
```

then type Ctl+x, then y to save, the type the name file1.txt and hit enter.

See that you made a file called file1.txt with:

```bash
ls
```
Let's do it again. This time we'll create a file with a name right off the bat:
```bash
nano file2.txt
```

Inside nano type:

```text

2
Dogs
```

then Ctl+x, y, enter. You should now be back at your command prompt.

Let's create one more text file that we'll do more things with in a minute. Use nano to create file3.txt and type the numbers 1-15 on separate lines in the text file, then save the file.

<br><br>

**Viewing the contents of a text file**

**head** - print the first 10 lines of the file to stdout

**tail** - print the last 10 lines of the file to stdout

**cat** - concatenate and print all contents to stdout

Let's try these out:

```bash
head file3.txt

tail file3.txt

cat file3.txt

cat file1.txt file2.txt file3.txt
```

The last command should concatenate the printing of all three files to stdout.

<br><br>

**Using cat to create textfiles**

**cat** - concatenate and print all contents to stdout.

You can actually use cat to create quick textfiles without using nano.
```bash
cat > file30.txt
```
After you issue this command you'll see your cursor move to the next line in the terminal. Type whatever you want, multiple lines if you want. Cat will put whatever you type into a file called file30.txt. When you are finished typing you must type Ctl+d at least once to execute the command.

An ```ls``` should now show that you've created file1.txt, file2.txt, file3.txt, and file30.txt

<br><br>

**Wildcards**

\* - matches at least one character

? - matches a single character

\[] - matches the characters inside the brackets

To better understand how to use wildcards we'll practice with ```ls```:

List all files that start with "file" and end with ".txt":
```bash
ls file*.txt
```

List all the files that start with "file", have one character after that, and end with ".txt"
```bash
ls file?.txt
```
Notice how this did not list file30.txt because the ? matched only a single character. You can however use two single character wildcards in a row:
```bash
ls file??.txt
```
should list only your file30.txt

Let's get fancier now by using the brackets:
```bash
ls file[1-3].txt
```
and you will get the same result with
```bash
ls file[1-3].*
```
which should list only file1.txt, file2.txt, and file3.txt

What do you think the following will list?
```bash
ls file[1,3].txt
```
You should see only file1 and file3.

How about this?
```bash
ls file[1,3]*
```
This command will list all files that start with "file" and have a 1 or a 3 somewhere in the filename.

<br><br>

**Copying and moving files**

**cp** - copy files and directories

**mv** - move or rename files

Make a copy of one of your text files:
```bash
cp file1.txt file1a.txt
```
Now list your files to see the new file you created and then view the contents of your new file with the head command.

Let's make a copy of one of your files to a different directory:
```bash
cp file1.txt ../file1.txt
```
You should now have a copy of file1 one level back in your home directory.

From your mynewdirectory, list the files in your home directory:
```bash
ls ~
```
You should see the copy of file1.txt in your home directory.

Let's move file2 into your home directory:
```bash
mv file2.txt ~
```

List the files in your home directory to see that file2 was moved.

Let's move file2.txt from your home directory back into your mynewdirectory:
```bash
mv ../file2.txt .
```
Note how the double dot indicates that the file is located one directory up and the single dot indicates the current working directory.

Let's rename file2:
```bash
mv file2.txt file100.txt
```

List the files in your working directory to see that you've renamed the file.

<br><br>

**search files for a word/phrase ("pattern")**

**grep** - print lines of files that match a pattern

Let's search our files in the mynewdirectory for the word "dog":
```bash
grep 'dog' *
```
You should see that two of your files contain the word "dog". But wait, only two?

```bash
grep -i 'dog' *
```
The -i flag means "ignore case" and will find 'dog' a third time. Notice how the word in the file is "Dogs" but grep found it anyway.

To search on an exact word:
```bash
grep -i -w 'dogs' *
```

<br><br>

**removing files and directories**

**rm** - remove files or directories

NOTE: there is no "undo" in linux. Be very careful when using the rm command. **When you delete something, it's gone- there's no trashcan to recover from.**

Delete one of your files with:
```bash
rm file100.txt
```
List your files to see that file100 is gone.


Let's remove a directory next. First make a new directory and move one of your files there:
```bash
mkdir delete-me
mv file1.txt delete-me
```

Try to remove the delete-me directory with the rm command:
```bash
rm delete-me
```

You should get an error. Overcome that error with:
```bash
rm -r delete-me
```

The directory delete-me should now be gone. What's the -r option mean?
```bash
man rm
```
We can see on the man page for rm that -r means remove directories and their contents recursively.

<br><br>

**change file permissions**

**chmod** - change file "mode"

If want to change who has access to certain files and directories, we use chmod (remember though that your home directory is private by default). View the permissions on your files with:
```bash
ls -l
```

You should see that the file owner, and group members can read read and write to these files, but everyone else ("others") only has read privileges. To add write privileges for everyone else:
```bash
chmod o+w file3.txt
ls -l
```

Now let's take away all privileges for "others":
```bash
chmod o-rw file3.txt
ls -l
```

<br><br>

**see how much storage you are using**

**du** - estimate file space usage

**my_quotas** - see a summary of your storage quotas and how much of the quotas you've used. only works from your home directory

In your mynewdirectory:
```bash
du -h
```

To list the size of each directory inside your home directory:
```bash
cd
du -h --max-depth=1
```
The max depth flag is often very helpful, otherwise you will get the size of every single file in every single directory.

To see a summary of your storage quotas and how much space you've used from each quota, go to your home directory and type:
```bash
my_quotas
```
This is the exact same info that appears when you SSH to Ceres. Note that this command only works from your home directory.

<br><br>

**download files/data to Ceres from the web**

**wget** - for downloading files from the web. It's non-interactive which means you can start something downloading with wget, log off the system before the download finishes, and the download will still be running. wget is not the only way to grab files from the web, but it's the only one we'll cover

Let's get one of my favorite gridded surface air temperature datasets from Berkeley Earth at http://berkeleyearth.org/data-new/. It's the global monthly land + ocean data on a 1-degree grid.

```bash
salloc
wget http://berkeleyearth.lbl.gov/auto/Global/Gridded/Land_and_Ocean_LatLong1.nc
exit
ls -lh
```

We just downloaded a 407MB data file in a few seconds. Wow, that was easy! More on salloc in a minute...

<br><br>

**view metadata of a netcdf file**

**ncdump** - converts netcdf file metadata to text form on stdout

just a quick side note on working with the data file we downloaded...

```bash
ncdump -h Land_and_Ocean_LatLong1.nc
```

This shows metadata on all the data variables that are in the files including dimension information for each data variable (lat, lon, time). There is also metadata about the file history.

Note: the -h flag means "header", without it you will get thousands of data values printed to stdout. If you do this by accident on a large file Ctl+c will stop the printing to stdout.

View the values of a single data variable with:
```bash
ncdump -v latitude Land_and_Ocean_LatLong1.nc
```

NetCDF is one of my favorite data formats because the spatial and temporal metadata is already "attached" to the data variables in the file. This means that when you go to plot 'temperature' from this netcdf file (using python, r, etc) the data will appear automagically on a map without you having to define the longitudes and latitudes.

Let's all delete this data file so that it's not sitting on Ceres in 50 different user accounts taking up space. Plus, it's easy to download again if we ever wanted to.
```bash
rm Land_and_Ocean_LatLong1.nc
```

<br><br>
-->

<p><strong>loading software from the module system</strong></p>

<p>view available software on Ceres with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module avail
</code></pre></div></div>

<p>load software (in this case conda) from the module system:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load miniconda
</code></pre></div></div>

<p>see what software modules you have loaded with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module list
</code></pre></div></div>

<p>unload software from the module system with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module unload miniconda
</code></pre></div></div>

<p><br /><br /></p>

<p><strong>There are also some SLURM-specific commands that are very useful</strong></p>

<p>See the <a href="https://bioinformaticsworkbook.org/Appendix/Unix/01_slurm-basics.html#gsc.tab=0">bioinformatics workbook</a> for more than what we cover here.</p>

<p><strong>sinfo</strong> - see the status of all the nodes</p>

<p><strong>salloc</strong> - “SLURM allocate”. Move onto a compute node in an interactive session. More in the next section.</p>

<p><strong>squeue</strong> - view information on compute jobs that are running. More in the next section.</p>

<p><strong>scancel</strong> - terminate a compute job that is running</p>

<p><strong>sbatch</strong> - submit a batch script to run a compute job. More in the next section</p>

<p><br /></p>

<p><a href="#session-2-tutorial">return to top of page</a>
<br /><br /></p>

<hr />

<h2 id="interactive-computing-vs-batch-computing">Interactive Computing vs Batch Computing</h2>

<p>Many geospatial researchers spend much of their time writing and debugging new scripts for each project they work on. This differs from other research communities who may be able to re-use a single script frequently for multiple projects or who can use a standard analysis process/workflow on many different input datasets.</p>

<p>A geospatial researcher may write and debug their scripts using small to medium size data until the script is functional and then modify the script to process big data only once. For this reason, geospatial researchers may more often use interactive computing sessions on an HPC.</p>

<h3 id="interactive-computing-on-ceres">Interactive Computing on Ceres</h3>

<p>Interactive computing essentially means that you are working directly on a compute node as opposed to using the SLURM job scheduler to submit compute jobs in batches. JupyterHub allows us easy access to interactive computing on the Ceres HPC. Just login to Ceres through JupyerHub and start coding in a Jupyter notebook- you will automatically be placed in an interactive compute session.</p>

<p>But let’s learn how to open an interactive computing session from the command line. This is important when you log in with SSH or if you’ve logged in with JupyterHub but want to compute or install software on a different node than where your JupyterLab session is running.</p>

<p><strong>Step 1: Open a terminal on Ceres</strong></p>

<p>Since we are already in JupyterLab, use the launcher to open a terminal. We could also use Windows Powershell or Mac/Linux Terminal to SSH onto the Ceres login node instead.</p>

<p><strong>Step 2: Start an Interactive Compute Session</strong></p>

<p>The simplest way to start an interactive session is:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>salloc
</code></pre></div></div>

<p>Issuing this command requests the SLURM job scheduler to allocate you a single hyper-threaded core (2 logical cores) with 6200 MB of allocated memory on one of the compute nodes. The session will last for 2 days, but will timeout after 1.5 hours of inactivity.</p>

<p>View your runnning compute sessions/jobs with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>squeue <span class="nt">-u</span> firstname.lastname
</code></pre></div></div>

<p>To exit the interactive session:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">exit</span>
</code></pre></div></div>

<p>For more control over your interactive session you can use the <code class="language-plaintext highlighter-rouge">srun</code> command instead of <code class="language-plaintext highlighter-rouge">salloc</code> using the format:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun <span class="nt">--pty</span> <span class="nt">-p</span> queuename <span class="nt">-t</span> hh:mm:ss <span class="nt">-n</span> cores <span class="nt">-N</span> nodes /bin/bash <span class="nt">-l</span>
</code></pre></div></div>

<p>for example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun <span class="nt">--pty</span> <span class="nt">-p</span> short <span class="nt">-t</span> 03:00:00 <span class="nt">-n</span> 4 <span class="nt">-N</span> 1 /bin/bash <span class="nt">-l</span>
</code></pre></div></div>

<p>will request the SLURM job scheduler to allocate you two hyper-threaded cores (4 logical cores) with 3100x4 MB of allocated memory on one of the compute nodes in the short partition. The session will last for 3 hours, but will also timeout due to inactivity after 1.5 hours.</p>

<h3 id="batch-computing-on-ceres">Batch Computing on Ceres</h3>

<p>Batch computing involves writing and executing a batch script that the SLURM job scheduler will manage. This mode of computing is good for “set it and forget it” compute jobs such as running a climate model, executing a single script multiple times in a row, or executing a more complicated but fully functional workflow that you know you don’t have to debug. We’ll cover how to write and execute a batch script next.</p>

<p><br /><br /></p>

<hr />

<h2 id="submitting-a-compute-job-with-a-slurm-batch-script">Submitting a Compute Job with a SLURM Batch Script</h2>

<p>Let’s practice by submitting a batch script.</p>

<p>First create simple python program:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> hello-world.py
print<span class="o">(</span><span class="s1">'Hello, world!'</span><span class="o">)</span>
</code></pre></div></div>

<p>then type Ctl-d to exit.</p>

<p>View the file you just created:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>hello-world.py
</code></pre></div></div>

<p><br /><br /></p>

<p><strong>a serial job that runs a python script one time</strong></p>

<p>Now create your batch script with nano or other text editor:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nano my-first-batch-script.sbatch
</code></pre></div></div>

<p>In the nano editor type:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --job-name=HelloWorld</span>
<span class="c">#SBATCH -p short              #name of the partition (queue) you are submitting to</span>
<span class="c">#SBATCH -N 1                  #number of nodes in this job</span>
<span class="c">#SBATCH -n 2                  #number of cores/tasks in this job</span>
<span class="c">#SBATCH -t 00:00:30           #time allocated for this job hours:mins:seconds</span>
<span class="c">#SBATCH -o "stdout.%j.%N"     # standard output, %j adds job number to output file name and %N adds the node name</span>
<span class="c">#SBATCH -e "stderr.%j.%N"     #optional, prints our standard error</span>

module load python_3
<span class="nb">echo</span> <span class="s2">"you are running python"</span>
python3 <span class="nt">--version</span>

python3 hello-world.py
</code></pre></div></div>

<p>Exit the nano editor with Ctl+O, enter, Ctl+X.</p>

<p>Submit your batch script with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch my-first-batch-script.sbatch
</code></pre></div></div>

<p>Check out the output of your compute job. It’s in the stdout file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls
cat </span>stdout.######.ceres##-compute-##
</code></pre></div></div>

<p>Note: there are a ton of other SBATCH options you could add to your script. For example, you could receive an email when your job has completed (<a href="https://scinet.usda.gov/guide/ceres/#batch-mode">see the Ceres User Manual</a>) and lots more (<a href="https://slurm.schedmd.com/sbatch.html">see the SLURM sbatch doc</a>).</p>

<p>Also Note: <strong>this is a serial job</strong>, meaning that it will run on a single compute core. The compute likely won’t be any faster than if you ran this type of job on your laptop. To run your hello-world code in parallel from a batch script (multiple times simulataneously on different cores) you would use openMP or MPI (see the <a href="https://scinet.usda.gov/guide/ceres/#running-a-simple-openmp-job">Ceres User Manual</a>) and your code would have to be in C or Fortran (not Python). For Python coders, there are much easier ways to run in parallel (using interactive mode as opposed to batch scripting), which we will cover in Session 3: Intro to Python and Dask.</p>

<p><br /><br /></p>

<p><strong>a serial job that runs a python script five times</strong></p>

<p>Let’s now run a script that will execute the same python code 5 times in a row back to back.</p>

<p>First, delete all your stdout and stderr files so it’s easier to see which new files have been generated:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">rm </span>std<span class="k">*</span>
</code></pre></div></div>

<p>Now modify your sbatch script using nano to look like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --job-name=HelloWorld</span>
<span class="c">#SBATCH -p short              #name of the partition (queue) you are submitting to</span>
<span class="c">#SBATCH -N 1                  #number of nodes in this job</span>
<span class="c">#SBATCH -n 2                  #number of cores/tasks in this job</span>
<span class="c">#SBATCH -t 00:00:30           #time allocated for this job hours:mins:seconds</span>
<span class="c">#SBATCH -o "stdout.%j.%N"     # standard output, %j adds job number to output file name and %N adds the node name</span>
<span class="c">#SBATCH -e "stderr.%j.%N"     #optional, prints our standard error</span>

module load python_3
<span class="nb">echo</span> <span class="s2">"you are running python"</span>
python3 <span class="nt">--version</span>

<span class="k">for </span>i <span class="o">{</span>1..5<span class="o">}</span>
<span class="k">do
  </span>python3 hello-world.py
<span class="k">done</span>
</code></pre></div></div>

<p>Look at a stdout file and you will see the python code ran 5 times.</p>

<p>Go ahead and delete your stdout and stderr files again.</p>

<p><br /><br /></p>

<p><strong>a parallel job that runs a python script 10 times simultaneously on different cores</strong></p>

<p>Let’s now run a script that will execute the same python code 10 times simulataneously. Modify your sbatch script to look like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash
#SBATCH --job-name=HelloWorld
#SBATCH -p short              #name of the partition (queue) you are submitting to
#SBATCH -N 1                 #number of nodes in this job
#SBATCH -n 10                 #number of cores/tasks in this job
#SBATCH --ntasks-per-core=1   
#SBATCH -t 00:00:30           #time allocated for this job hours:mins:seconds
#SBATCH -o "stdout.%j.%N"     # standard output, %j adds job number to output file name and %N adds the node name
#SBATCH -e "stderr.%j.%N"     #optional, prints our standard error
#SBATCH --array=1-10          #job array index values

module load python_3
echo "you are running python"
python3 --version

python3 hello-world.py
</code></pre></div></div>

<p>You should see a stdout and stderr file for each job in the array of jobs that were run (10) and and the jobs should have run on a variety of different nodes.</p>

<p><br /></p>

<p><a href="#session-2-tutorial">return to top of page</a>
<br /><br /></p>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      

    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form onsubmit="return googleCustomSearchExecute();" id="cse-search-box-form-id">
    <input type="search" id="cse-search-input-box-id" aria-placeholder="Enter your search term..." class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    </form>
    <div id="results" class="results">
        <gcse:searchresults-only></gcse:searchresults-only>
    </div></div>

      </div>
    
    
    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->

        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="https://twitter.com/isugif"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="https://github.com/https://github.com/isugenomics"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Geospatial Workbook</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.13/js/all.js"></script>


<script>
  (function () {
    var cx = '009853197685285203469:nsvri1pa88d';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();

  function googleCustomSearchExecute() {
    var input = document.getElementById('cse-search-input-box-id');
    var element = google.search.cse.element.getElement('searchresults-only0');
    if (input.value == '') {
      element.clearAllResults();
    } else {
      element.execute(input.value);
    }
    return false;
  }

  
</script>




<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" defer
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>




  </body>
</html>
