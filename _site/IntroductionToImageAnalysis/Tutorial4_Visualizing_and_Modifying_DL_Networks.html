<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.11.2 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Premeeting - Geospatial Workbook</title>
<meta name="description" content="Tutorial on Informatics for Geospatial Information">


  <meta name="author" content="Kerrie Geil">


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Geospatial Workbook">
<meta property="og:title" content="Premeeting">
<meta property="og:url" content="http://localhost:4000/IntroductionToImageAnalysis/Tutorial4_Visualizing_and_Modifying_DL_Networks.html">




  <meta property="og:image" content="http://localhost:4000/assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg">



  <meta name="twitter:site" content="@isugif">
  <meta name="twitter:title" content="Premeeting">
  <meta name="twitter:description" content="Tutorial on Informatics for Geospatial Information">
  <meta name="twitter:url" content="http://localhost:4000/IntroductionToImageAnalysis/Tutorial4_Visualizing_and_Modifying_DL_Networks.html">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="http://localhost:4000/assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg">
  

  
    <meta name="twitter:creator" content="@someone">
  







  

  


<link rel="canonical" href="http://localhost:4000/IntroductionToImageAnalysis/Tutorial4_Visualizing_and_Modifying_DL_Networks.html">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Geospatial Workbook Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E6BZVYF8ZY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E6BZVYF8ZY');
</script>

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Geospatial Workbook</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/about.html" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/list.html" >Index</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/glossary.html" >Glossary</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/people.html" >People</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/contributing.html" >Contribute</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  











<div class="page__hero--overlay"
  style="background-color: 444444; background-image: url('/assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Premeeting

        
      </h1>
      
      
      
    </div>
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/people/KerrieGeil.png" alt="Kerrie Geil" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Kerrie Geil</h3>
    
    
      <p class="author__bio" itemprop="description">
        Kerrie is an ARS SCINet postdoc in the research group of Dr. Deb Peters in Las Cruces, NM. Her M.S. and Ph.D. degrees are in Atmospheric Sciences and her research background is in climate modeling.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      
        <li>
          <a href="mailto:mailto:someone@iastate.edu">
            <meta itemprop="email" content="mailto:someone@iastate.edu" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      
        <li>
          <a href="https://twitter.com/someone" itemprop="sameAs">
            <i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

<!-- Create a 2nd author for tutorials -->



<div itemscope itemtype="http://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Premeeting">
    
    
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
        <h1 id="tutorial-4-visualizing-and-modifying-dl-networks">Tutorial 4: Visualizing and Modifying DL Networks</h1>
<h2 id="laura-e-boucheron-electrical--computer-engineering-nmsu">Laura E. Boucheron, Electrical &amp; Computer Engineering, NMSU</h2>
<h3 id="october-2020">October 2020</h3>
<p>Copyright (C) 2020  Laura E. Boucheron</p>

<p>This information is free; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.</p>

<p>This work is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p>

<p>You should have received a copy of the GNU General Public License along with this work; if not, If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.</p>

<h2 id="overview">Overview</h2>
<p>In this tutorial, we pick up with the trained MNIST Network from Tutorial 2 and explore some ways of probing the characteristics of the trained network to help us debug common pitfalls in adapting network architectures.Â </p>

<p>This tutorial contains 5 sections:</p>
<ul>
  <li><strong>Section 0: Preliminaries</strong>: some notes on using this notebook, how to download the image dataset that we will use for this tutorial, and import commands for the libraries necessary for this tutorial</li>
  <li><strong>Section 1: Printing Characteristics of the CNN</strong> how to print textual summaries of the CNN architecture</li>
  <li><strong>Section 2: Visualizing Activations</strong> how to filter an example image through thhe MNIST network and visualize the activations</li>
  <li><strong>Section 3: Inputting New and Different Data to the Network</strong> how to process new data to be compatible with the MNIST network and the effects of showing a non-digit image to the network</li>
  <li><strong>Section 4: The VGG Network</strong> an exploration of the VGG16 network.</li>
</ul>

<p>There are a few subsections with the heading â<strong><span style="color:Green"> Your turn: </span></strong>â throughout this tutorial in which you will be asked to apply what you have learned.</p>

<p>Portions of this tutorial have been taken or adapted from https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/ and the documentation at https://keras.io.</p>

<h1 id="section-0-preliminaries">Section 0: Preliminaries</h1>
<h2 id="a-note-on-jupyter-notebooks">A Note on Jupyter Notebooks</h2>

<p>There are two main types of cells in this notebook: code and markdown (text).  You can add a new cell with the plus sign in the menu bar above and you can change the type of cell with the dropdown menu in the menu bar above.  As you complete this tutorial, you may wish to add additional code cells to try out your own code and markdown cells to add your own comments or notes.</p>

<p>Markdown cells can be augmented with a number of text formatting features, including</p>
<ul>
  <li>bulleted</li>
  <li>lists</li>
</ul>

<p>embedded $\LaTeX$, monotype specification of <code class="language-plaintext highlighter-rouge">code syntax</code>, <strong>bold font</strong>, and <em>italic font</em>.  There are many other features of markdown cellsâsee the jupyter documentation for more information.</p>

<p>You can edit a cell by double clicking on it.  If you double click on this cell, you can see how to implement the various formatting referenced above.  Code cells can be run and markdown cells can be formatted using Shift+Enter or by selecting the Run button in the toolbar above.</p>

<p>Once you have completed (all or part) of this notebook, you can share your results with colleagues by sending them the <code class="language-plaintext highlighter-rouge">.ipynb</code> file.  Your colleagues can then open the file and will see your markdown and code cells as well as any results that were printed or displayed at the time you saved the notebook.  If you prefer to send a notebook without results displayed (like this notebook appeared when you downloaded it), you can select (âRestart &amp; Clear Outputâ) from the Kernel menu above.  You can also export this notebook in a non-executable form, e.g., <code class="language-plaintext highlighter-rouge">.pdf</code> through the File, Save As menu.</p>

<h2 id="section-01-downloading-images">Section 0.1 Downloading Images</h2>
<p>Download the <code class="language-plaintext highlighter-rouge">my_digits1_compressed.jpg</code> and <code class="language-plaintext highlighter-rouge">latest_256_0193.jpg</code> files available on the workshop webpage.  We will use those images in Sections 3 and 4 of this tutorial.</p>

<p>We will also use the <code class="language-plaintext highlighter-rouge">cameraman.png</code> and <code class="language-plaintext highlighter-rouge">peppers.png</code> files that we used in Tutorial 1 and the CalTech101 dataset that we used in Tutorial 2.</p>

<h2 id="section-02a-import-necessary-libraries-for-users-using-a-local-machine">Section 0.2a Import Necessary Libraries (For users using a local machine)</h2>
<p>Here, at the top of the code, we import all the libraries necessary for this tutorial.  We will introduce the functionality of any new libraries throughout the tutorial, but include all import statements here as standard coding practice.  We include a brief comment after each library here to indicate its main purpose within this tutorial.</p>

<p>It would be best to run this next cell before the workshop starts to make sure you have all the necessary packages installed on your machine.</p>

<p>A few other notes:</p>
<ul>
  <li>After the first import of keras packages, you may get a printout in a pink box that states
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using Theano backend
</code></pre></div>    </div>
    <p>or</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using TensorFlow backend
</code></pre></div>    </div>
  </li>
  <li>You may get one or more warnings complaining about various configs.  As long as you donât get any errors, you should be good to go.  You can, if you wish, fix whatever is causing a warning at a later point in time.  I find it best to copy and paste the error warning itself into a Google search and tack on the OS in which you encountered the error.  Seldom have I encountered an error that someone else hasnât encountered in my same OS.</li>
  <li>The third to the last line in the following code cell imports the MNIST dataset.</li>
  <li>The last two lines load the VGG16 network and the weights for that network trained on the ImageNet dataset.  The code below will load the VGG16 network, trained on ImageNet.  The first time this code is run, the trained network will be downloaded.  Subsequent times, the trained network will be loaded from the local disk.  This network is very large (528 MB) as we will see shortly, so it may take some time to download.  Generally, we would include the last line below as part of our code rather than imports, but we include it here to allow that download to complete before the workshop.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> <span class="c1"># mathematical and scientific functions
</span><span class="kn">import</span> <span class="nn">imageio</span> <span class="c1"># image reading capabilities
</span><span class="kn">import</span> <span class="nn">skimage.color</span> <span class="c1"># functions for manipulating color images
</span><span class="kn">import</span> <span class="nn">skimage.transform</span> <span class="c1"># functions for transforms on images
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span> <span class="c1"># visualization
</span>
<span class="c1"># format matplotlib options
</span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">'font.size'</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span>

<span class="kn">import</span> <span class="nn">keras.backend</span> <span class="c1"># information on the backend that keras is using
</span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span> <span class="c1"># a generic keras model class used to modify architectures
</span><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">np_utils</span> <span class="c1"># functions to wrangle label vectors
</span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span> <span class="c1"># the basic deep learning model
</span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Convolution2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span> <span class="c1"># important CNN layers
</span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span> <span class="c1"># to load a pre-saved model (may require hdf libraries installed)
</span><span class="kn">from</span> <span class="nn">keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">load_img</span> <span class="c1"># keras method to read in images 
</span><span class="kn">from</span> <span class="nn">keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">img_to_array</span> <span class="c1"># keras method to convert images to numpy array
</span><span class="kn">from</span> <span class="nn">keras.applications.vgg16</span> <span class="kn">import</span> <span class="n">preprocess_input</span> <span class="c1"># keras method to transform images to VGG16 expected characteristics
</span><span class="kn">from</span> <span class="nn">keras.applications.vgg16</span> <span class="kn">import</span> <span class="n">decode_predictions</span> <span class="c1"># keras method to present highest ranked categories
</span><span class="kn">from</span> <span class="nn">keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span> <span class="c1"># framework to input batches of images into keras
</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span> <span class="c1"># the MNIST dataset
</span><span class="kn">from</span> <span class="nn">keras.applications</span> <span class="kn">import</span> <span class="n">vgg16</span> <span class="c1"># the VGG network
</span><span class="n">model_vgg16</span> <span class="o">=</span> <span class="n">vgg16</span><span class="p">.</span><span class="n">VGG16</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">)</span> <span class="c1"># download the ImageNet weights for VGG16
</span></code></pre></div></div>

<h2 id="section-02b-build-the-conda-environment-for-users-using-the-ars-hpc-ceres-with-jupyterlab">Section 0.2b Build the Conda Environment (For users using the ARS HPC Ceres with JupyterLab)</h2>
<p>Open a terminal from inside JupyterLab (File &gt; New &gt; Terminal) and type the following commands</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source activate
wget https://kerriegeil.github.io/NMSU-USDA-ARS-AI-Workshops/aiworkshop.yml
conda env create --prefix /project/your_project_name/envs/aiworkshop -f aiworkshop.yml
</code></pre></div></div>
<p>This will build the environment in one of your project directories. It may take 5 minutes to build the Conda environment.</p>

<p>See https://kerriegeil.github.io/NMSU-USDA-ARS-AI-Workshops/setup/ for more information.</p>

<p>When the environment finishes building, select this environment as your kernel in your Jupyter Notebook (click top right corner where you see Python 3, select your new kernel from the dropdown menu, click select)</p>

<p>You will want to do this BEFORE the workshop starts.</p>

<h2 id="section-04-load-your-trained-mnist-model">Section 0.4 Load Your Trained MNIST Model</h2>
<p>At the end of Tutorial 3 we saved the trained MNIST model <code class="language-plaintext highlighter-rouge">model1</code> in <code class="language-plaintext highlighter-rouge">model1.h5</code>.  Here will load that model and we can pick up right where we left off.</p>

<p>If you were not able to save the model at the end of Tutorial 3, you can re-run the training of the MNIST model here before we start the rest of the tutorial.  For your convenience, below is the complete code that will load and preprocess the MNIST data and define and train the model.  You can cut and paste the code here into a code cell in this notebook and run it.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from keras.datasets import mnist
from keras.utils import np_utils
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D
model1 = Sequential()
model1.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))
model1.add(Convolution2D(32, (3, 3), activation='relu'))
model1.add(MaxPooling2D(pool_size=(2,2)))
model1.add(Flatten())
model1.add(Dense(128, activation='relu'))
model1.add(Dense(10, activation='softmax'))
model1.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
model1.fit(X_train, Y_train, batch_size=64, epochs=1, verbose=1)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s">'model1.h5'</span><span class="p">)</span>
</code></pre></div></div>

<p>We have now loaded the trained MNIST model from Tutorial 3.  Since this is a new notebook, however, we do not have the actual MNIST data loaded. We copy the code for loading and preprocessing the MNIST data from the Tutorial 3 notebook.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">/=</span> <span class="mi">255</span>
<span class="n">X_test</span> <span class="o">/=</span> <span class="mi">255</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">np_utils</span><span class="p">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">np_utils</span><span class="p">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="section-1-printing-characteristics-of-the-cnn">Section 1: Printing Characteristics of the CNN</h1>

<h2 id="11-the-summary-method">1.1 The <code class="language-plaintext highlighter-rouge">summary</code> method</h2>
<p>The <code class="language-plaintext highlighter-rouge">summary</code> method of a keras model will display a basic text summary of the CNN architecture with layer name, layer type, output shape, and number of parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="a-note-about-the-none-value-in-shape">A note about the <code class="language-plaintext highlighter-rouge">None</code> value in shape</h3>
<p>The <code class="language-plaintext highlighter-rouge">None</code> value in the output shapes is used as a placehoder before the network knows how many samples it will be processing.</p>

<h3 id="using-tab-compete-to-explore-attributes-and-methods">Using tab-compete to explore attributes and methods</h3>
<p>The tab-complete feature of <code class="language-plaintext highlighter-rouge">ipython</code> can be very helpful to explore the available attributes and methods for a variable.  There are useful attributes and methods for the model <code class="language-plaintext highlighter-rouge">model1</code> and for the layers in the model, accessed with the <code class="language-plaintext highlighter-rouge">layers</code> attribute of the model.</p>

<h2 id="-your-turn-"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Explore the attributes and methods of the model variable <code class="language-plaintext highlighter-rouge">model1</code> by placing your cursor after the <code class="language-plaintext highlighter-rouge">.</code> and pressing tab.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">help</span><span class="p">(</span><span class="n">model1</span><span class="p">.)</span>
</code></pre></div></div>

<h2 id="-your-turn--1"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Explore the attributes and methods of the layers in <code class="language-plaintext highlighter-rouge">model1</code>.  Change the index into <code class="language-plaintext highlighter-rouge">model1.layers</code> in the first code cell and run that cell to access the different CNN layers.  Then, place your cursor after the <code class="language-plaintext highlighter-rouge">.</code> and press tab.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer</span> <span class="o">=</span> <span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">help</span><span class="p">(</span><span class="n">layer</span><span class="p">.)</span>
</code></pre></div></div>

<h2 id="12-a-layer-wise-summary-of-input-and-output-shapes">1.2 A layer-wise summary of input and output shapes</h2>
<p>While the <code class="language-plaintext highlighter-rouge">summary</code> method of the model prints some useful information, there are additional pieces of information that can be very useful.  Below is a function definition  which will print a layer-wise summary of the input and output shapes.  This information can be very helpful in helping to understand and debug the workings (or non-workings) of the model.  This code loops over each layer in the model using the <code class="language-plaintext highlighter-rouge">layers</code> attribute of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_shapes</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Layer Name</span><span class="se">\t\t</span><span class="s">Type</span><span class="se">\t\t</span><span class="s">Input Shape</span><span class="se">\t\t</span><span class="s">Output Shape</span><span class="se">\t</span><span class="s">Trainable'</span><span class="p">)</span><span class="c1"># print column headings
</span>    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>  <span class="c1"># loop over layers
</span>        <span class="n">lname</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">name</span> <span class="c1"># grab layer name
</span>        <span class="n">ltype</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">layer</span><span class="p">).</span><span class="n">__name__</span> <span class="c1"># grab layer type
</span>        <span class="n">ltype</span><span class="p">[</span><span class="n">ltype</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">'/'</span><span class="p">):]</span> <span class="c1"># parse for only the last part of the string
</span>        <span class="k">if</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'Conv2D'</span><span class="p">:</span> <span class="c1"># print for convolutional layers
</span>            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">input_shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span>\
                  <span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">output_shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'MaxPooling2D'</span><span class="p">:</span> <span class="c1"># print for maxpool layers
</span>            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">input_shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span>\
                  <span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">output_shape</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'Flatten'</span><span class="p">:</span> <span class="c1"># print for flatten layers
</span>            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">input_shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span>\
                  <span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">output_shape</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'Dense'</span><span class="p">:</span> <span class="c1"># print for dense layers
</span>            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">input_shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span>\
                  <span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">output_shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">))</span>
</code></pre></div></div>

<p>We can print a summary of the input and output shapes by passing <code class="language-plaintext highlighter-rouge">model1</code> to the <code class="language-plaintext highlighter-rouge">print_shapes</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print_shapes</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="-your-turn--2"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Does this summary reconcile with the discussion in Tutorial 3 about the architecture of the MNIST model?  You might find it helpful to refer to the Tutorial 3 slides with the visualization of the MNIST network.</p>

<h2 id="13-a-layer-wise-summary-of-filter-shape-and-parameters">1.3 A layer-wise summary of filter shape and parameters</h2>
<p>Below is a function definition which will print a layer-wise summary of the filters and parameters.  This information can also be very helpful in helping to understand and debug the workings (or non-workings) of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_params</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initialize counter for total params
</span>    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initialize counter for trainable params
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'Layer Name</span><span class="se">\t\t</span><span class="s">Type</span><span class="se">\t\t</span><span class="s">Filter shape</span><span class="se">\t\t</span><span class="s"># Parameters</span><span class="se">\t</span><span class="s">Trainable'</span><span class="p">)</span> <span class="c1"># print column headings
</span>    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span> <span class="c1"># loop over layers
</span>        <span class="n">lname</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">name</span> <span class="c1"># grab layer name
</span>        <span class="n">ltype</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">layer</span><span class="p">).</span><span class="n">__name__</span> <span class="c1"># grab layer type
</span>        <span class="n">ltype</span><span class="p">[</span><span class="n">ltype</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">'/'</span><span class="p">):]</span> <span class="c1"># parse for only the last part of the string
</span>        <span class="k">if</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'Conv2D'</span><span class="p">:</span> <span class="c1"># print for convolutional layers
</span>            <span class="n">weights</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>
            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span>\
                  <span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">count_params</span><span class="p">())</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">.</span><span class="n">count_params</span><span class="p">()</span>
            <span class="n">total_params</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">.</span><span class="n">count_params</span><span class="p">()</span> <span class="c1"># update number of params
</span>        <span class="k">elif</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'MaxPooling2D'</span><span class="p">:</span> <span class="c1"># print for max pool layers
</span>            <span class="n">weights</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>
            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">---------------</span><span class="se">\t\t</span><span class="s">---'</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'Flatten'</span><span class="p">:</span> <span class="c1"># print for flatten layers
</span>            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">---------------</span><span class="se">\t\t</span><span class="s">---'</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">ltype</span><span class="o">==</span><span class="s">'Dense'</span><span class="p">:</span> <span class="c1"># print for dense layers
</span>            <span class="n">weights</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>
            <span class="k">print</span><span class="p">(</span><span class="n">lname</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t\t</span><span class="s">'</span><span class="o">+</span><span class="n">ltype</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="se">\t\t</span><span class="s">'</span><span class="o">+</span>\
                  <span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">count_params</span><span class="p">())</span><span class="o">+</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="p">:</span>
                <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">.</span><span class="n">count_params</span><span class="p">()</span>
            <span class="n">total_params</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">.</span><span class="n">count_params</span><span class="p">()</span> <span class="c1"># update number of params
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'---------------'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Total trainable parameters: '</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">trainable_params</span><span class="p">))</span> <span class="c1"># print total params
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'Total untrainable parameters: '</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">total_params</span><span class="o">-</span><span class="n">trainable_params</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Total parameters: '</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">total_params</span><span class="p">))</span>
</code></pre></div></div>

<p>We can print a summary of the input and output shapes by passing <code class="language-plaintext highlighter-rouge">model1</code> to the <code class="language-plaintext highlighter-rouge">print_shapes</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print_params</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="check-out-the-total-number-of-parameters">Check out the total number of parameters!!!</h3>
<p>The MNIST model that we trained yesterday has <strong>more than 600,000 parameters</strong> that it learned during training!  We will see later in this tutorial that is is actually a very small network.</p>

<p>We note a few things about the number of parameters per layer:</p>
<ul>
  <li>The second conv layer has a lot more parameters than the first.  That is due to the fact that the second conv layer filters across all 32 channels of the activations from the first conv layer.</li>
  <li>The max pool and flatten layers donât have any parameters.</li>
  <li>The fully connected (dense) layers are the source of a large proportion of the total parameters in this network.</li>
</ul>

<h2 id="-your-turn--3"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>What implications does the number of trainable parameters per layer have on transfer learning and decisions about which layers to freeze?  There is a code cell below prepopluated with the code to clone <code class="language-plaintext highlighter-rouge">model1</code> and to freeze all but the last layer for you to modify and explore using the <code class="language-plaintext highlighter-rouge">print_params</code> and <code class="language-plaintext highlighter-rouge">print_shapes</code> functions defined above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1_clone</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">clone_model</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
<span class="n">model1_clone</span><span class="p">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">model1</span><span class="p">.</span><span class="n">get_weights</span><span class="p">())</span>

<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model1_clone</span><span class="p">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="o">=</span><span class="bp">False</span>
</code></pre></div></div>

<h3 id="a-note-on-number-of-parameters-per-layer">A note on number of parameters per layer</h3>
<p>This cell describes a few more details of how you can reconcile the filter shape and the total number of parameters.  The uninterested reader can skip this section without impeding their ability to complete the rest of the tutorial.</p>

<p>Recall that a basic neuron has a set of weights and a bias.  The parameters that must be learned in deep learning layers include both the weights and biases.  We break down the computation for convolutional layers and for fully connected (dense) layers.  We will use the same notation as from the Tutorial 3 slides.</p>

<h4 id="convolutional-layers">Convolutional layers:</h4>
<p>Each filter in a convolutional layer has $K\cdot K\cdot C$ weights and one bias, where $K$ is the kernel size and $C$ is the number of channels.  Thus we have a total of $M_{conv}\cdot K\cdot K\cdot C$ weights and $M_{conv}$ biases, where $M_{conv}$ is the number of filters in the layer.  This means a total number of trainable parameters of $M_{conv}(K^2C+1)$.</p>

<h4 id="fully-connected-layers">Fully connected layers:</h4>
<p>Each node in a fully connected layer is connected to every node in the previous layer and we thus have $M_{FC}^{(i-1)}$ weights and one bias per node, where $M_{FC}^{(i-1)}$ is the number of weights in the previous fully connected (or flattened) layer.  Thus we have a total of $M_{FC}^{(i)}\cdot M_{FC}^{(i-1)}$ weights and $M_{FC}^{(i)}$ biases, where $M_{FC}^{(i)}$ is the number of nodes in the current fully connected layer.  This means we have a total number of trainable parameters of $M_{FC}^{(i)}(M_{FC}^{(i-1)}+1)$.</p>

<h1 id="section-2-visualizing-activations">Section 2 Visualizing Activations</h1>
<p>In this section we will explore means to visualize the activations in different layers throughout the network.</p>

<h2 id="section-21-wrangling-the-example-input-image-dimensions">Section 2.1 Wrangling the example input image dimensions</h2>
<p>The responses (activations) for each filter in a layer can be computed by sending an example image through the network and requesting that the network report the output at the layer of interest (rather than at the output layer).</p>

<p>First, we need to choose an image to filter through the network.  It is this image for which the activations will be computed.  We begin here with the first test image.  Recall that the network expects a tensor in the form samples$\times28\times28\times1$.  In this case, weâll be providing only one sample, so we need our input to be $1\times28\times28\times1$.</p>

<p>The following code reshapes the zeroth test image with is shape $28\times28\times1$ into a tensor of shape $1\times28\times28\times1$ where the leading dimension of 1 is just wrangling the dimensionality to the samples$\times28\times28\times1$ format expected of an input tensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_example</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>We can plot the image to give us an idea of the appearance of the original image.  This will allow us to better analyze the filtered images that we will see when we plot the activations.  Since we have added some extra dimensions to this image, we use the <code class="language-plaintext highlighter-rouge">np.squeeze</code> function to remove those dimensions with only one entry before sending it to <code class="language-plaintext highlighter-rouge">plt.imshow</code>.  In this case, the <code class="language-plaintext highlighter-rouge">np.squeeze</code> function returns a $28\times28$ <code class="language-plaintext highlighter-rouge">numpy</code> array.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">X_example</span><span class="p">),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="-your-turn--4"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Explore the dimensionalities of <code class="language-plaintext highlighter-rouge">X_example</code> relative to <code class="language-plaintext highlighter-rouge">X_test[0]</code> and <code class="language-plaintext highlighter-rouge">np.squeeze(X_example)</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="section-22-modify-model-to-output-activations-after-the-first-conv-layer">Section 2.2 Modify <code class="language-plaintext highlighter-rouge">model</code> to output activations after the first conv layer</h2>
<p>Now, we modify our <code class="language-plaintext highlighter-rouge">model1</code> to output the activations after the first convolutional layer.  We use the generic <code class="language-plaintext highlighter-rouge">Model</code> class from <code class="language-plaintext highlighter-rouge">keras</code> and specify the same inputs as <code class="language-plaintext highlighter-rouge">model1</code>, but specify the output to be after the zeroth layer.  This is where the <code class="language-plaintext highlighter-rouge">print_shapes</code> and <code class="language-plaintext highlighter-rouge">print_params</code> functions can be very helpful to determine which layer you actually want to specify as output. We call this new model <code class="language-plaintext highlighter-rouge">model1_layer0</code> to designate that it is the same as <code class="language-plaintext highlighter-rouge">model</code>, but outputing information after layer 0, i.e., the first convolutional layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1_layer0</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p>Now if we ask for the prediction of the model for <code class="language-plaintext highlighter-rouge">X_example</code>, the model will output the activations at the first convolutional layer instead of the activations at the final softmax layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer0_activations</span> <span class="o">=</span> <span class="n">model1_layer0</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="-your-turn--5"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Based on our textual summaries of this network, we expect that the output should be of shape $1\times26\times26\times32$.  Check the dimensionality and variable type of <code class="language-plaintext highlighter-rouge">layer0_activations</code>, as well as the intensity range.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="section-23-visualizing-the-32-activations-of-the-first-conv-layer">Section 2.3 Visualizing the 32 activations of the first conv layer</h2>
<p>We can loop over the 32 activations and plot each.  <code class="language-plaintext highlighter-rouge">plt.imshow</code> will, by default, choose an intensity range to match that of the input image.  This can make it difficult to compare between activations: a bright pixel in one image might actually be more activated than in another.  We can force the plots to be on the same scale of intensities by passing the minimum and maximum intensities to <code class="language-plaintext highlighter-rouge">plt.imshow</code> using the <code class="language-plaintext highlighter-rouge">vmin</code> and <code class="language-plaintext highlighter-rouge">vmax</code> options.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">min_int</span> <span class="o">=</span> <span class="n">layer0_activations</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span> <span class="c1"># find min intensity for all activations
</span><span class="n">max_int</span> <span class="o">=</span> <span class="n">layer0_activations</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span> <span class="c1"># find max intensity for all activations
</span><span class="n">subplot_rows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer0_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># determine subplots rows
</span><span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">layer0_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="c1"># loop over filters
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># choose current subplot
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">layer0_activations</span><span class="p">[:,:,:,</span><span class="n">f</span><span class="p">]),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span>\
               <span class="n">vmin</span><span class="o">=</span><span class="n">min_int</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">max_int</span><span class="p">)</span> <span class="c1"># plot activations
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
</code></pre></div></div>

<p>We see that there are some filters that respond to the entire digit, some that respond only the the horizontal stroke of the digit, some that respond only to the vertical stroke, and we may even see some that donât respond at all.  These filters that donât respond may be tuned for shapes (e.g., curves) that donât appear in the digit 7.</p>

<h2 id="-your-turn--6"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Explore the change in visualization if you do not use the <code class="language-plaintext highlighter-rouge">vmin</code> and <code class="language-plaintext highlighter-rouge">vmax</code> options above.  For your convenience, the code from the cell above is copied below for you to modify.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">min_int</span> <span class="o">=</span> <span class="n">layer0_activations</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span> <span class="c1"># find min intensity for all activations
</span><span class="n">max_int</span> <span class="o">=</span> <span class="n">layer0_activations</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span> <span class="c1"># find max intensity for all activations
</span><span class="n">subplot_rows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer0_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># determine subplots rows
</span><span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">layer0_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="c1"># loop over filters
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># choose current subplot
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">layer0_activations</span><span class="p">[:,:,:,</span><span class="n">f</span><span class="p">]),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span>\
               <span class="n">vmin</span><span class="o">=</span><span class="n">min_int</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">max_int</span><span class="p">)</span> <span class="c1"># plot activations
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="section-24-modify-model-to-visualize-activations-after-the-second-conv-layer">Section 2.4 Modify model to visualize activations after the second conv layer</h2>
<p>We can look at the activations of the second convolutional layer with a simple modification of the code above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1_layer1</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer1_activations</span> <span class="o">=</span> <span class="n">model1_layer1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">min_int</span> <span class="o">=</span> <span class="n">layer1_activations</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span> <span class="c1"># find min intensity for all activations
</span><span class="n">max_int</span> <span class="o">=</span> <span class="n">layer1_activations</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span> <span class="c1"># find max intensity for all activations
</span><span class="n">subplot_rows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer1_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># determine subplots rows
</span><span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">layer1_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="c1"># loop over filters
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># choose current subplot
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">layer1_activations</span><span class="p">[:,:,:,</span><span class="n">f</span><span class="p">]),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span>\
               <span class="n">vmin</span><span class="o">=</span><span class="n">min_int</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">max_int</span><span class="p">)</span> <span class="c1"># plot activations
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
</code></pre></div></div>

<p>We see that the second layer filters have gotten more specific in the structures to which they are responding.  This is consistent with what we know about the hierarchical nature of feature learning in CNNs.</p>

<h2 id="section-25-modify-model-to-visualize-activations-after-the-max-pool-layer">Section 2.5 Modify model to visualize activations after the max pool layer</h2>
<h2 id="-your-turn--7"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Modify the code above to visualize the output after the max pool layer.  For your convenience, the code from the cell above is copied below for you to modify.  Noteâyou probably want to define a new variable for this model to avoid overwriting the other models from above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1_layer1</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer1_activations</span> <span class="o">=</span> <span class="n">model1_layer1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">min_int</span> <span class="o">=</span> <span class="n">layer1_activations</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span> <span class="c1"># find min intensity for all activations
</span><span class="n">max_int</span> <span class="o">=</span> <span class="n">layer1_activations</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span> <span class="c1"># find max intensity for all activations
</span><span class="n">subplot_rows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer1_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># determine subplots rows
</span><span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">layer1_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="c1"># loop over filters
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># choose current subplot
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">layer1_activations</span><span class="p">[:,:,:,</span><span class="n">f</span><span class="p">]),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span>\
               <span class="n">vmin</span><span class="o">=</span><span class="n">min_int</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">max_int</span><span class="p">)</span> <span class="c1"># plot activations
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
</code></pre></div></div>

<p>We note that the activations of the max pool layer are simply lower resolution representations of the second convolutional layer activations.</p>

<h2 id="section-26-modify-model-to-visualize-activations-of-the-fully-connected-layers">Section 2.6 Modify model to visualize activations of the fully connected layers</h2>
<p>While the output of the flattened and fully connected layers are not images, we can visualize the activations by treating them like a one-row image.  This can give us some insight into which neurons are responding the most to the digit 7.</p>
<h3 id="the-flattened-layer">The flattened layer</h3>
<p>Since the dimensions of the flattened layer are $1\times4608$, we need to âstretchâ out the pixels to actually be able to see them.  We use the <code class="language-plaintext highlighter-rouge">aspect</code> parameter in <code class="language-plaintext highlighter-rouge">plt.imshow</code> to do this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1_layer3</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer3_activations</span> <span class="o">=</span> <span class="n">model1_layer3</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">layer3_activations</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># plot filter coeffs
</span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This visualization may not be particularly elucidating, but we include it here for the sake of completeness.  Note that this $1\times4608$ vector of activations is just a reshaping of the $32\times12\times12=4608$ pixels in the max pool activations.</p>

<h3 id="the-first-fully-connected-layer">The first fully connected layer</h3>
<p>Since the dimensions of the first fully connected layer is only $1\times128$, we donât need to mess with the aspect ratio of the <code class="language-plaintext highlighter-rouge">plt.imshow</code> visualization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1_layer4</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">4</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer4_activations</span> <span class="o">=</span> <span class="n">model1_layer4</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">layer4_activations</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span> <span class="c1"># plot filter coeffs
</span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This visualization can be interpreted in some sense as some aggregate of features that this layer is cueing on from the max pool layer.  We expect that different of these neurons will activate for different digits.</p>

<h3 id="the-second-fully-connected-layer">The second fully connected layer</h3>
<p>Note that the second fully connected layer is also the softmax output layer.  We leave the axis labels on here for more easy determination of which digit(s) the network is claiming probability for.  We also put grid lines on the image to even better delineate the different digits.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model1_layer5</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">5</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer5_activations</span> <span class="o">=</span> <span class="n">model1_layer5</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">layer5_activations</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span> <span class="c1"># plot filter coeffs
#plt.axis('off')
</span><span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="s">'on'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>We notice that the output here is a very high confidence in the digit 7 and very little in other digits.  This is consistent with the interpretation of the softmax output layer if we were to look at the actual probability values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">layer5_activations</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="-your-turn--8"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Explore the activations of the network for other input images.  For your convenience, code cells from above have been copied here for you to modify.</p>

<h3 id="defining-a-specific-input-image">Defining a specific input image</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_example</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Original image'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">X_example</span><span class="p">),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="output-of-the-first-convolutional-layer">Output of the first convolutional layer</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'First convolutional layer'</span><span class="p">)</span>
<span class="n">model1_layer0</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer0_activations</span> <span class="o">=</span> <span class="n">model1_layer0</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">min_int</span> <span class="o">=</span> <span class="n">layer0_activations</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span> <span class="c1"># find min intensity for all activations
</span><span class="n">max_int</span> <span class="o">=</span> <span class="n">layer0_activations</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span> <span class="c1"># find max intensity for all activations
</span><span class="n">subplot_rows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer0_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># determine subplots rows
</span><span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">layer0_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="c1"># loop over filters
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># choose current subplot
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">layer0_activations</span><span class="p">[:,:,:,</span><span class="n">f</span><span class="p">]),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span>\
               <span class="n">vmin</span><span class="o">=</span><span class="n">min_int</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">max_int</span><span class="p">)</span> <span class="c1"># plot filter coeffs
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="second-convolutional-layer">Second convolutional layer</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Second convolutional layer'</span><span class="p">)</span>
<span class="n">model1_layer1</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer1_activations</span> <span class="o">=</span> <span class="n">model1_layer1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">min_int</span> <span class="o">=</span> <span class="n">layer1_activations</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span> <span class="c1"># find min intensity for all activations
</span><span class="n">max_int</span> <span class="o">=</span> <span class="n">layer1_activations</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span> <span class="c1"># find max intensity for all activations
</span><span class="n">subplot_rows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">layer1_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># determine subplots rows
</span><span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">layer1_activations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="c1"># loop over filters
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">subplot_rows</span><span class="p">,</span><span class="n">f</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># choose current subplot
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">layer1_activations</span><span class="p">[:,:,:,</span><span class="n">f</span><span class="p">]),</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span>\
               <span class="n">vmin</span><span class="o">=</span><span class="n">min_int</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">max_int</span><span class="p">)</span> <span class="c1"># plot filter coeffs
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="the-flattened-layer-1">The flattened layer</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'The flattened layer'</span><span class="p">)</span>
<span class="n">model1_layer3</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer3_activations</span> <span class="o">=</span> <span class="n">model1_layer3</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">layer3_activations</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span><span class="n">aspect</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span> <span class="c1"># plot filter coeffs
</span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="the-first-fully-connected-layer-1">The first fully connected layer</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'First fully connected layer'</span><span class="p">)</span>
<span class="n">model1_layer4</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">4</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer4_activations</span> <span class="o">=</span> <span class="n">model1_layer4</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">layer4_activations</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span> <span class="c1"># plot filter coeffs
</span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="the-second-fully-connected-layer-the-output-layer">The second fully connected layer (the output layer)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Second fully connected layer (output layer)'</span><span class="p">)</span>
<span class="n">model1_layer5</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">model1</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">5</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
<span class="n">layer5_activations</span> <span class="o">=</span> <span class="n">model1_layer5</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_example</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">layer5_activations</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span> <span class="c1"># plot filter coeffs
</span><span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h1 id="section-3-inputting-new-and-different-data-to-the-trained-network">Section 3 Inputting New and Different Data to the Trained Network</h1>
<p>In this section, weâll explore the use of this trained network to operate on new data.  We will use the <code class="language-plaintext highlighter-rouge">my_digits1_compressed.jpg</code> image provided as part of this tutorial.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">I</span> <span class="o">=</span> <span class="n">imageio</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'my_digits1_compressed.jpg'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">I</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Note that image is an RGB image of 10 handwritten digits.  You will wrangle this image into a format suitable for input to the MNIST network in this section.</p>

<h2 id="-your-turn--9"><strong><span style="color:Green"> Your turn: </span></strong></h2>

<p>You should have noticed that image is an RGB image of 10 handwritten digits.  Use what you have learned in Tutorials 1, 2 and 3 to extract each of those 10 digits from the image and get it in the correct form to input to the MNIST network.</p>

<p>As a reminder you probably want to pay attention to:</p>
<ul>
  <li>RGB versus gray</li>
  <li>Variable type</li>
  <li>Intensity range (Hintâyou can invert the intensities and have light digits on a dark background by subtracting the image from the maximum intensity.)</li>
  <li>Cropping indices (Hintârows 295 through 445 and columns 1160 through 1310 will crop the digit 0)</li>
  <li>Resizing</li>
  <li>Correct tensor dimensions: recall that the network expects a tensor in the form samples$\times28\times28\times1$.  In this case, youâll be providing only one sample, so you will need your input to be $1\times28\times28\times1$.</li>
</ul>

<p>Use your extracted digits as input to the MNIST network <code class="language-plaintext highlighter-rouge">model1</code>.  Does the network predict the correct label for the digit?  What does the predicted softmax label tell you about the confidence in the prediction for this new image?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p>This is a small demonstration of the ability for this network to correctly classify data from an entirely new source.  We note, however, that the preparation of the data is critical for this success.  If we were to pre-process the data in a manner not designed for the MNIST network, we might get very different results.</p>

<h2 id="-your-turn--10"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Repeat the above analysis, but keep the digit as dark on a light background.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p>By keeping the background light and the digit dark, the network interprets the background as the digit.  It is not actually processing the information in the same way we interpret this image as the digit we see.</p>

<h1 id="section-4-the-vgg16-network">Section 4: The VGG16 Network</h1>
<p>In this section, we will use what we have learned about deep learning and image processing to explore a common CNN network called VGG16.  The VGG network is described in this paper: https://arxiv.org/abs/1409.1556 and is a common architecture for image classification.  This network was trained to classify 1000 categories (https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).</p>

<p>NoteâVGG16 is some 528 MB to download.  This is a typical size for state-of-the-art CNNs.  We actually downloaded these weights when we imported libraries (local machines) or activated the conda environment (HPC).</p>

<h2 id="section-41-loading-the-vgg-network-trained-on-imagenet">Section 4.1 Loading the VGG Network Trained on ImageNet</h2>
<p><code class="language-plaintext highlighter-rouge">keras</code> includes functions to load a VGG16 network with additional options to download a âpretrainedâ networkâi.e., one that has been trained on ImageNet.  ImageNet is a database of millions of images (see http://www.image-net.org/) spanning thousands of categories.</p>

<p>The code below will load the VGG16 network, trained on ImageNet.  The first time this code is run, the trained network will be downloaded.  Subsequent times, the trained network will be loaded from the local disk.  This network is very large as we will see shortly, so it may take some time to download.</p>

<p>Similar to how we saved our MNIST model at the end of Tutorial 3 and loaded it at the beginning of this tutorial, we are loading a VGG16 model that has already been trained on the millions of images and 1000 categories of ImageNet.  It is no trivial task to train a network the size of VGG16 (weeks on a multiple GPUs), so we want to leverage the work that has already been done.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_vgg</span> <span class="o">=</span> <span class="n">vgg16</span><span class="p">.</span><span class="n">VGG16</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">)</span>
</code></pre></div></div>

<p>We can use <code class="language-plaintext highlighter-rouge">print_shapes</code> and <code class="language-plaintext highlighter-rouge">print_params</code> to explore the structure of the VGG16 model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print_shapes</span><span class="p">(</span><span class="n">model_vgg</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print_params</span><span class="p">(</span><span class="n">model_vgg</span><span class="p">)</span>
</code></pre></div></div>

<p>There are over 138 million parameters in this network!!!  It has many more layers than the simple MNIST network that we have been working with.</p>

<h2 id="section-42-classification-capabilities-of-the-vgg16-network">Section 4.2: Classification Capabilities of the VGG16 Network</h2>
<p>So, what happens if we show a new image to this network?  Letâs see what happens if we show it the <code class="language-plaintext highlighter-rouge">cameraman.png</code> image.<br />
This example is adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e.</p>

<p>In the code below, we are leveraging many built-in <code class="language-plaintext highlighter-rouge">keras</code> functions, including</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">load_img</code> from <code class="language-plaintext highlighter-rouge">keras</code> to read in an image while resizing to the expected input size of $224\times224$</li>
  <li><code class="language-plaintext highlighter-rouge">preprocess_input</code> specific to the VGG16 model in <code class="language-plaintext highlighter-rouge">keras</code> to scale the intensities to the expected range.  Iâm unsure of the details of this, but I do know that itâs a process more complicated than a simple intensity scaling (since it results in negative intensities).  Further details are likely in the VGG paper (https://arxiv.org/abs/1409.1556), but can be difficult to find sometimes.</li>
  <li><code class="language-plaintext highlighter-rouge">decode_predictions</code> specific to the VGG16 model in <code class="language-plaintext highlighter-rouge">keras</code> to find the top three confidences and map those to the class labels</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e
</span>
<span class="c1"># load an image from file
</span><span class="n">image</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">(</span><span class="s">'cameraman.png'</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="c1"># convert the image pixels to a numpy array
</span><span class="n">image</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># reshape data for the model
</span><span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="c1"># prepare the image for the VGG model
</span><span class="n">image</span> <span class="o">=</span> <span class="n">preprocess_input</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># predict the probability across all output classes
</span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model_vgg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># convert the probabilities to class labels
</span><span class="n">label</span> <span class="o">=</span> <span class="n">decode_predictions</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
<span class="c1"># retrieve the most likely result, e.g. highest probability
</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">labelk</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
    <span class="c1"># print the classification
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'%s (%.2f%%)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">labelk</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labelk</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div></div>

<p>We notice that the networkâs performance on this image is pretty decent, specifying that it believes the image is one of a âtripod.â  There is not a âcameramanâ category in ImageNet (https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), so the network chose the most likely category from the available ones.</p>

<p>Here, we repeat the above for the <code class="language-plaintext highlighter-rouge">peppers.png</code> image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e
</span>
<span class="c1"># load an image from file
</span><span class="n">image</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">(</span><span class="s">'peppers.png'</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="c1"># convert the image pixels to a numpy array
</span><span class="n">image</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># reshape data for the model
</span><span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="c1"># prepare the image for the VGG model
</span><span class="n">image</span> <span class="o">=</span> <span class="n">preprocess_input</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># predict the probability across all output classes
</span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model_vgg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># convert the probabilities to class labels
</span><span class="n">label</span> <span class="o">=</span> <span class="n">decode_predictions</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
<span class="c1"># retrieve the most likely result, e.g. highest probability
</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">labelk</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
    <span class="c1"># print the classification
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'%s (%.2f%%)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">labelk</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labelk</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div></div>

<p>We find that the networkâs performance on this model is also very good, specifying that the image is of a âbell pepperâ.</p>

<p>Both the cameraman and peppers image contain objects similar to those that the VGG16 network encountered in the ImageNet database.  As such, it does a remarkably good job of classifying those images.</p>

<p>What happens if the network encounters something really different than what itâs seen before?  What if you show it an image of something not included in the 1000 classes of objects?  The image <code class="language-plaintext highlighter-rouge">latest_256_0193.jpg</code> image is an image of the Sun at a wavelength of 193 angsgtroms from NASAâs Solar Dynamics Observatory satellite (https://sdo.gsfc.nasa.gov/data/)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e
</span>
<span class="c1"># load an image from file
</span><span class="n">image</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">(</span><span class="s">'latest_256_0193.jpg'</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="c1"># convert the image pixels to a numpy array
</span><span class="n">image</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># reshape data for the model
</span><span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="c1"># prepare the image for the VGG model
</span><span class="n">image</span> <span class="o">=</span> <span class="n">preprocess_input</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># predict the probability across all output classes
</span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model_vgg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># convert the probabilities to class labels
</span><span class="n">label</span> <span class="o">=</span> <span class="n">decode_predictions</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
<span class="c1"># retrieve the most likely result, e.g. highest probability
</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">labelk</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
    <span class="c1"># print the classification
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'%s (%.2f%%)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">labelk</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labelk</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>

<span class="n">I</span> <span class="o">=</span> <span class="n">imageio</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'latest_256_0193.jpg'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'latest_256_0193.jpg'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>The network is torn between classifying this image of the sun as a âtickâ, âFrench loafâ, or a ânailâ.  Hmmmâ¦. Things arenât looking so good anymore.  But we must remember that we never showed the network ground truth of the sun in 193 angstroms during training.  We canât really expect that it can jump to that conclusion.</p>

<h2 id="-your-turn--11"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>What class does the VGG16 network think your data belong to?  If you donât have data with you, you can peruse the internet for images of something that you want to try classifying with the network.  Just save the image to the same directory as this notebook and use the code above to classify it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="-your-turn--12"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Using an image of your choice, use the methods we learned above to explore the workings of the VGG16 model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="43-transfer-learning-on-the-vgg16-architecture">4.3 Transfer Learning on the VGG16 Architecture</h2>

<p>Here we show an example of how to perform transfer learning on the VGG16 architecture using the CalTech101 dataset as the new input data.</p>

<p>Similar to how we modified the model to output activations at certain layers bove, we can truncate the VGG16 model to any desired layer and then add on additional layers at our discretion.  In this case, since we noted relatively good performance of the basic VGG16 architecture on images of similar appearance to the object categories in ImageNet, we expect that we wonât need to change too much of the VGG16 architecture.  In this example, we choose to modify only the final prediction layer.</p>

<p>It is also common practice to retrain on all the fully connected layers.  Generally speaking, the further in appearance your data are from the ImageNet images, the further back in the architecture you probably want to retrain.</p>

<p>In the following code, we keep the entire VGG16 architecture up until the last fully connected layer (which also happens to be the output layer) and define a new model <code class="language-plaintext highlighter-rouge">model2_vgg</code> which consists only of those layers we want to keep.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model2_vgg</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model_vgg</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span><span class="n">outputs</span><span class="o">=</span><span class="n">model_vgg</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we need to add in at least a new prediction layer as the final layer.  We could also add additional layers within the network if we thought they were needed.  Since the CalTech101 dataset has 101 classes, we need the final fully connected layer to have 101 nodes and a softmax activation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_output</span> <span class="o">=</span> <span class="n">model2_vgg</span><span class="p">.</span><span class="n">output</span> <span class="c1"># take the output as currently defined
</span><span class="n">new_output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">101</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)(</span><span class="n">new_output</span><span class="p">)</span> <span class="c1"># operate on that output with another dense layer
</span><span class="n">model2_vgg</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model2_vgg</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span><span class="n">outputs</span><span class="o">=</span><span class="n">new_output</span><span class="p">)</span> <span class="c1"># define a new model with the new output
</span></code></pre></div></div>

<p>Up to this point we have defined a new architecture, where we amputated the final fully connected layer and stitched back on a new one.  If we look at the layers of this new model using the modified <code class="language-plaintext highlighter-rouge">print_params</code> function,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print_params</span><span class="p">(</span><span class="n">model2_vgg</span><span class="p">)</span>
</code></pre></div></div>

<p>we see that all the layers have attribute <code class="language-plaintext highlighter-rouge">trainable</code> set to <code class="language-plaintext highlighter-rouge">True</code>.  This means that if we were to train this new model, we will train all 138 million parameters.  We donât want to do this.</p>

<p>We want to âfreezeâ the parameters for all layers except that new one that we added.  To do this, we set the <code class="language-plaintext highlighter-rouge">trainable</code> attribute of all layers we wish to freeze to <code class="language-plaintext highlighter-rouge">False</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model2_vgg</span><span class="p">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span><span class="o">=</span><span class="bp">False</span>
</code></pre></div></div>

<p>Now if we check the trainability of the layers, we see that all layers except the final layer are frozen, and we will be training (learning) only some 413,797 parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print_params</span><span class="p">(</span><span class="n">model2_vgg</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we need to point out new architecture at the CalTech101 data in order to train that final layer.  We again use built-in functions in <code class="language-plaintext highlighter-rouge">keras</code> to handle the flow of data through the network.  With MNIST we could fit all the training images in one array in memory and grab batches from there.  With a larger dataset like CalTech101, we begin to lose our ability to fit everything in memory and instead will read image from the specified directory into batches at training time.</p>

<p>The code below uses an <code class="language-plaintext highlighter-rouge">ImageDataGenerator</code> class from keras and defines the preprocessing applied to that image as the <code class="language-plaintext highlighter-rouge">preprocess_input</code> function we already used above.  Recall that this is a preprocessing function defined specifically for the VGG16 network.</p>

<p>Next, we use the <code class="language-plaintext highlighter-rouge">flow_from_directory</code> function of the <code class="language-plaintext highlighter-rouge">ImageDataGenerator</code> to define a flow of images from a specified directory. It is assumed that the specified directory contains a subdirectory for each class.  Other options specified for this function are</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">target_size</code> which specifies the spatial dimensions to which all input data will be resized</li>
  <li><code class="language-plaintext highlighter-rouge">color_mode</code> which specifies that these images should be treated as RGB images.  These built-in functions are nice to use since they (often, not always) have niceties associated with taking care of things like converting grayscale images to the correct dimensionality.</li>
  <li><code class="language-plaintext highlighter-rouge">batch_size</code> the number of images to process per batch in the training</li>
  <li><code class="language-plaintext highlighter-rouge">class_mode</code> which specifies that this is a multi-class classification problem</li>
  <li><code class="language-plaintext highlighter-rouge">shuffle</code> which specifies that the batches will be selected randomly rather than in alphanumerical order</li>
</ul>

<p>There are other options available, see <code class="language-plaintext highlighter-rouge">help(train_datagen.flow_from_directory)</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">preprocessing_function</span><span class="o">=</span><span class="n">preprocess_input</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">train_datagen</span><span class="p">.</span><span class="n">flow_from_directory</span><span class="p">(</span><span class="s">'101_ObjectCategories'</span><span class="p">,</span>\
                                                    <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">),</span> <span class="n">color_mode</span><span class="o">=</span><span class="s">'rgb'</span><span class="p">,</span>\
                                                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">class_mode</span><span class="o">=</span><span class="s">'categorical'</span><span class="p">,</span>\
                                                    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we compile the model and specify the same options as we used for the MNIST network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model2_vgg</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>

<p>Now we can train the model.  Since we donât have the entire training data to feed to the training, we instead invoke the <code class="language-plaintext highlighter-rouge">fit_generator</code> method which can utilize the <code class="language-plaintext highlighter-rouge">train_generator</code> we define above.  Additionally, the <code class="language-plaintext highlighter-rouge">fit_generator</code> takes a <code class="language-plaintext highlighter-rouge">steps_per_epoch</code> option rather than a <code class="language-plaintext highlighter-rouge">batch_size</code> option.  We define the <code class="language-plaintext highlighter-rouge">steps_per_epoch</code> as the number of images over which we will train divided by the batch size.</p>

<p>This code will take a while.  It took about 30 minutes per epoch on 6 4.8 GHz Intel i9 processors.  In most applications, we would train this over multiple epochs to boost the accuracy even higher.  Here we train for one epoch to limit the computational time.  After one epoch, the network reported accuracy above 80%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">step_size_train</span> <span class="o">=</span> <span class="n">train_generator</span><span class="p">.</span><span class="n">n</span><span class="o">//</span><span class="n">train_generator</span><span class="p">.</span><span class="n">batch_size</span> <span class="c1"># the // does a floor after division
</span><span class="n">model2_vgg</span><span class="p">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">train_generator</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">step_size_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="-your-turn--13"><strong><span style="color:Green"> Your turn: </span></strong></h2>
<p>Using an image of your choice from CalTech101, use the methods we learned above to explore the workings of the new transfer-learned VGG16 model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      

    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form onsubmit="return googleCustomSearchExecute();" id="cse-search-box-form-id">
    <input type="search" id="cse-search-input-box-id" aria-placeholder="Enter your search term..." class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    </form>
    <div id="results" class="results">
        <gcse:searchresults-only></gcse:searchresults-only>
    </div></div>

      </div>
    
    
    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->

        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="https://twitter.com/isugif"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="https://github.com/https://github.com/isugenomics"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Geospatial Workbook</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.13/js/all.js"></script>


<script>
  (function () {
    var cx = '009853197685285203469:nsvri1pa88d';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();

  function googleCustomSearchExecute() {
    var input = document.getElementById('cse-search-input-box-id');
    var element = google.search.cse.element.getElement('searchresults-only0');
    if (input.value == '') {
      element.clearAllResults();
    } else {
      element.execute(input.value);
    }
    return false;
  }

  
</script>




<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" defer
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>




  </body>
</html>
